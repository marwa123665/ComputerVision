{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectAndDescribe(image):\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        sift = cv2.xfeatures2d.SIFT_create()\n",
    "        (kps, features) = sift.detectAndCompute(gray, None)\n",
    "        \n",
    "        #kps = np.float32([kp.pt for kp in kps])\n",
    "        return (kps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeMatches(a_images,b_images):\n",
    "\n",
    "    for imageA, imageB in zip(a_images, b_images):\n",
    "        #imageA = cv2.imread(img_a)\n",
    "        #imageB = cv2.imread(img_b)\n",
    "\n",
    "        imageA = cv2.resize(imageA,(400,200))\n",
    "        imageB = cv2.resize(imageB,(400,200))\n",
    "\n",
    "        (kpsA, descriptorA) = detectAndDescribe(imageA)\n",
    "        (kpsB, descriptorB) = detectAndDescribe(imageB)\n",
    "\n",
    "        # match features between the two images\n",
    "        matcher = cv2.BFMatcher()\n",
    "        rawMatches =matcher.knnMatch(descriptorA, descriptorB, 2)\n",
    "        matches_backward = matcher.knnMatch(descriptorB, descriptorA, 2)\n",
    "\n",
    "\n",
    "        matches = []\n",
    "        ratio = 0.7\n",
    "        \n",
    "        #David Lowe's ratio test and cross-checking\n",
    "        for m,n in rawMatches:\n",
    "            if m.distance<(ratio*n.distance):\n",
    "                for mm, nn in matches_backward:\n",
    "                    if mm.trainIdx == m.queryIdx and mm.queryIdx == m.trainIdx:\n",
    "                        matches.append(m)\n",
    "                        break\n",
    "        matchesMask=None\n",
    "        #apply the ransac mask to the matches    \n",
    "        if len(matches) >=4:\n",
    "            ptsA = np.float32([kpsA[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "            ptsB = np.float32([kpsB[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "            \n",
    "            (H, mask) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC,20)\n",
    "            \n",
    "            matchesMask = mask.ravel().tolist()\n",
    "            print(\"matchesMask: \", len(matchesMask))\n",
    "        \n",
    "       \n",
    "    \n",
    "        #3- Threshold based on distance. (The distance must not be constant and should be derived from an analysis of the data distribution)\n",
    "        # if len(matches) > 0:\n",
    "        #     distances = [m.distance for m in matches]\n",
    "            \n",
    "        #     median_distance = np.median(distances)\n",
    "        #     threshold = median_distance\n",
    "        #     matches = [m for m in matches if m.distance < threshold]    \n",
    "\n",
    "        #4- Draw the matches on the images and display the images\n",
    "\n",
    "\n",
    "        similarity_score = len(matches) / min(len(kpsA), len(kpsB))\n",
    "    \n",
    "        if similarity_score > 0.2:\n",
    "            print(\"The images are similar\")\n",
    "            \n",
    "        else:\n",
    "            print(\"The images are not similar\")\n",
    "        print(\"similarity score: \", similarity_score)\n",
    "        #result = cv2.drawMatches(imageA, kpsA, imageB, kpsB, matches, None)\n",
    "        #cv2.line(result, (0, 200), (400, 200), (0, 0, 255), 2)\n",
    "        #cv2.imshow(\"result\", result)\n",
    "        print(\"matches: \", len(matches))\n",
    "        if matchesMask!=None and len(matches)==len(matchesMask)  :   \n",
    "            result=cv2.drawMatches(imageA,kpsA,imageB,kpsB,matches,None,\n",
    "                            matchesMask=matchesMask,# draw only inliers\n",
    "                            flags=2)\n",
    "            cv2.imshow(\"result\", result)\n",
    "\n",
    "        cv2.imshow(\"imageA\",imageA)\n",
    "        cv2.imshow(\"imageB\",imageB)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/marwa/Downloads/CV Labs/assignment data-20240329T203755Z-001/assignment data\\\\image1a.jpeg', 'C:/Users/marwa/Downloads/CV Labs/assignment data-20240329T203755Z-001/assignment data\\\\image1b.jpeg', 'C:/Users/marwa/Downloads/CV Labs/assignment data-20240329T203755Z-001/assignment data\\\\image2a.jpeg', 'C:/Users/marwa/Downloads/CV Labs/assignment data-20240329T203755Z-001/assignment data\\\\image2b.jpeg', 'C:/Users/marwa/Downloads/CV Labs/assignment data-20240329T203755Z-001/assignment data\\\\image3a.jpeg', 'C:/Users/marwa/Downloads/CV Labs/assignment data-20240329T203755Z-001/assignment data\\\\image3b.jpeg', 'C:/Users/marwa/Downloads/CV Labs/assignment data-20240329T203755Z-001/assignment data\\\\image4a.jpeg', 'C:/Users/marwa/Downloads/CV Labs/assignment data-20240329T203755Z-001/assignment data\\\\image4b.jpeg', 'C:/Users/marwa/Downloads/CV Labs/assignment data-20240329T203755Z-001/assignment data\\\\image4c.png', 'C:/Users/marwa/Downloads/CV Labs/assignment data-20240329T203755Z-001/assignment data\\\\image5a.jpeg', 'C:/Users/marwa/Downloads/CV Labs/assignment data-20240329T203755Z-001/assignment data\\\\image5b.jpeg', 'C:/Users/marwa/Downloads/CV Labs/assignment data-20240329T203755Z-001/assignment data\\\\image6a.jpeg', 'C:/Users/marwa/Downloads/CV Labs/assignment data-20240329T203755Z-001/assignment data\\\\image6b.jpeg', 'C:/Users/marwa/Downloads/CV Labs/assignment data-20240329T203755Z-001/assignment data\\\\image7a.jpeg', 'C:/Users/marwa/Downloads/CV Labs/assignment data-20240329T203755Z-001/assignment data\\\\image7b.jpeg']\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) d:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0x16764f4e::Set<1,-1,-1>,struct cv::impl::A0x16764f4e::Set<0,2,5>,4>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m         ComputeMatches(imageB,imageC)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[43mComputeMatches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimageA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimageB\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[76], line 10\u001b[0m, in \u001b[0;36mComputeMatches\u001b[1;34m(a_images, b_images)\u001b[0m\n\u001b[0;32m      7\u001b[0m imageA \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(imageA,(\u001b[38;5;241m400\u001b[39m,\u001b[38;5;241m200\u001b[39m))\n\u001b[0;32m      8\u001b[0m imageB \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(imageB,(\u001b[38;5;241m400\u001b[39m,\u001b[38;5;241m200\u001b[39m))\n\u001b[1;32m---> 10\u001b[0m (kpsA, descriptorA) \u001b[38;5;241m=\u001b[39m \u001b[43mdetectAndDescribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimageA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m (kpsB, descriptorB) \u001b[38;5;241m=\u001b[39m detectAndDescribe(imageB)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# match features between the two images\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[75], line 2\u001b[0m, in \u001b[0;36mdetectAndDescribe\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetectAndDescribe\u001b[39m(image):\n\u001b[1;32m----> 2\u001b[0m         gray \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m         sift \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mxfeatures2d\u001b[38;5;241m.\u001b[39mSIFT_create()\n\u001b[0;32m      5\u001b[0m         (kps, features) \u001b[38;5;241m=\u001b[39m sift\u001b[38;5;241m.\u001b[39mdetectAndCompute(gray, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) d:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0x16764f4e::Set<1,-1,-1>,struct cv::impl::A0x16764f4e::Set<0,2,5>,4>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "images = glob.glob(\"C:/Users/marwa/Downloads/CV Labs/assignment data-20240329T203755Z-001/assignment data/*\")\n",
    "print(images)\n",
    "a_images = [img for img in images if img.endswith(\"a.jpeg\")]\n",
    "b_images = [img for img in images if img.endswith(\"b.jpeg\")]\n",
    "c_images = [img for img in images if img.endswith(\"c.png\")]\n",
    "\n",
    "a_images.sort()\n",
    "b_images.sort()\n",
    "c_images.sort()\n",
    "max_length = max(len(a_images), len(b_images), len(c_images))\n",
    "for i in range(max_length):\n",
    "    if i < len(a_images):\n",
    "        img_a = a_images[i]\n",
    "        imageA = cv2.imread(img_a,cv2.IMREAD_UNCHANGED)\n",
    "   \n",
    "\n",
    "    if i < len(b_images):\n",
    "        img_b = b_images[i]\n",
    "        imageB = cv2.imread(img_b,cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "\n",
    "    if i < len(c_images):\n",
    "        img_c = c_images[i]\n",
    "        imageC = cv2.imread(img_c,cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "\n",
    "# Process images\n",
    "\n",
    "    image_number_a = int(''.join(filter(str.isdigit, img_a)))\n",
    "    image_number_b = int(''.join(filter(str.isdigit, img_b)))\n",
    "    image_number_c = int(''.join(filter(str.isdigit, img_c)))\n",
    "    if image_number_a == image_number_b == image_number_c:\n",
    "\n",
    "            ComputeMatches(imageA,imageB)\n",
    "            ComputeMatches(imageA,imageC)\n",
    "            ComputeMatches(imageB,imageC)\n",
    "    else:\n",
    "        ComputeMatches(imageA,imageB)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
